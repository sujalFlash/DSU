{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-24T17:33:00.952585Z",
     "start_time": "2024-09-24T17:32:49.286173Z"
    }
   },
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True,trust_repo=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sujal/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:33:22.256727Z",
     "start_time": "2024-09-24T17:33:10.453503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"TCGA_CS_4944.png\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ],
   "id": "51c5a003ef872083",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:47:24.634803Z",
     "start_time": "2024-09-24T17:47:24.507923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "# Convert the image to a NumPy array for calculating mean and std\n",
    "input_image_np = np.array(input_image)\n",
    "\n",
    "# Calculate mean and std (for each channel)\n",
    "m, s = np.mean(input_image_np, axis=(0, 1)), np.std(input_image_np, axis=(0, 1))\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=m / 255.0, std=s / 255.0),  # Normalize expects values between 0 and 1\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Check if GPU is available and move data/model to GPU if so\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Print the rounded output\n",
    "print(torch.round(output[0]))\n"
   ],
   "id": "a750358ec414a311",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fn\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapper\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;129;43m@register_meta\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroi_align\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43mmeta_roi_align\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_ratio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maligned\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrois must have shape as Tensor[K, 5]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001B[0m, in \u001B[0;36mregister_meta.<locals>.wrapper\u001B[1;34m(fn)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(fn):\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension\u001B[49m\u001B[38;5;241m.\u001B[39m_has_ops():\n\u001B[0;32m     19\u001B[0m         get_meta_lib()\u001B[38;5;241m.\u001B[39mimpl(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mtorchvision, op_name), overload_name), fn)\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:02:48.691772Z",
     "start_time": "2024-09-25T08:02:48.653549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from time import time\n",
    "from nnunetv2.model_sharing.model_import import install_model_from_zip_file\n",
    "from nnunetv2.paths import nnUNet_results\n",
    "from tqdm import tqdm"
   ],
   "id": "878cf4025643ed23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:03:35.581792Z",
     "start_time": "2024-09-25T08:03:35.564693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_and_install_from_url(url):\n",
    "    assert nnUNet_results is not None, \"Cannot install model because network_training_output_dir is not \" \\\n",
    "                                                    \"set (RESULTS_FOLDER missing as environment variable, see \" \\\n",
    "                                                    \"Installation instructions)\"\n",
    "    print('Downloading pretrained model from url:', url)\n",
    "    import http.client\n",
    "    http.client.HTTPConnection._http_vsn = 10\n",
    "    http.client.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "\n",
    "    import os\n",
    "    home = os.path.expanduser('~')\n",
    "    random_number = int(time() * 1e7)\n",
    "    tempfile = join(home, f'.nnunetdownload_{str(random_number)}')\n",
    "\n",
    "    try:\n",
    "        download_file(url=url, local_filename=tempfile, chunk_size=8192 * 16)\n",
    "        print(\"Download finished. Extracting...\")\n",
    "        install_model_from_zip_file(tempfile)\n",
    "        print(\"Done\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        if isfile(tempfile):\n",
    "            os.remove(tempfile)"
   ],
   "id": "907ada2ce5350fbb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:03:54.422061Z",
     "start_time": "2024-09-25T08:03:54.409565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_file(url: str, local_filename: str, chunk_size: Optional[int] = 8192 * 16) -> str:\n",
    "    # borrowed from https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True, timeout=100) as r:\n",
    "        r.raise_for_status()\n",
    "        with tqdm.wrapattr(open(local_filename, 'wb'), \"write\", total=int(r.headers.get(\"Content-Length\"))) as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ],
   "id": "8492d3720fac9a5b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T07:57:04.518152Z",
     "start_time": "2024-09-26T07:57:03.239071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        self.decoder4 = self.upconv_block(512, 512)\n",
    "        self.decoder3 = self.upconv_block(512, 256)\n",
    "        self.decoder2 = self.upconv_block(256, 128)\n",
    "        self.decoder1 = self.upconv_block(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Debug print statements to trace the tensor shapes\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        enc1 = self.encoder1(x)\n",
    "        print(f\"After encoder1: {enc1.shape}\")\n",
    "        \n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2))\n",
    "        print(f\"After encoder2: {enc2.shape}\")\n",
    "        \n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2))\n",
    "        print(f\"After encoder3: {enc3.shape}\")\n",
    "        \n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2))\n",
    "        print(f\"After encoder4: {enc4.shape}\")\n",
    "        \n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc4, kernel_size=2))\n",
    "        print(f\"After bottleneck: {bottleneck.shape}\")\n",
    "        \n",
    "        dec4 = self.decoder4(bottleneck)\n",
    "        print(f\"After decoder4 (before concat): {dec4.shape}\")\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        print(f\"After decoder4 (after concat): {dec4.shape}\")\n",
    "        \n",
    "        dec3 = self.decoder3(dec4)\n",
    "        print(f\"After decoder3 (before concat): {dec3.shape}\")\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        print(f\"After decoder3 (after concat): {dec3.shape}\")\n",
    "        \n",
    "        dec2 = self.decoder2(dec3)\n",
    "        print(f\"After decoder2 (before concat): {dec2.shape}\")\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        print(f\"After decoder2 (after concat): {dec2.shape}\")\n",
    "        \n",
    "        dec1 = self.decoder1(dec2)\n",
    "        print(f\"After decoder1 (before concat): {dec1.shape}\")\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        print(f\"After decoder1 (after concat): {dec1.shape}\")\n",
    "        \n",
    "        output = self.final_conv(dec1)\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "# Initialize the model and load the weights\n",
    "model = UNet(in_channels=3, out_channels=1)  # Adjust channels as needed\n",
    "model.load_state_dict(torch.load('3channel50.pt', map_location=torch.device('cpu'),weights_only=True), strict=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ],
   "id": "a3f8f511e0ba3602",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder1): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T07:57:07.648124Z",
     "start_time": "2024-09-26T07:57:06.072009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_dicom_image(image_path):\n",
    "    # Load DICOM image using pydicom\n",
    "    dcm_data = pydicom.dcmread(image_path)\n",
    "\n",
    "    # Extract pixel data (might require reshaping depending on your model)\n",
    "    image_data = dcm_data.pixel_array\n",
    "\n",
    "    # Convert to RGB (if necessary, depending on your model)\n",
    "    image_data = Image.fromarray(image_data).convert('RGB')  # Convert to PIL Image for consistency\n",
    "    \n",
    "    # Resize the image if necessary\n",
    "    image_data = image_data.resize((256, 256))\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image_data)\n",
    "   \n",
    "\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Shape: (1, 3, 256, 256)\n",
    "    print(image_tensor.shape)\n",
    "    return image_tensor\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    \"\"\"Postprocesses the output tensor from a segmentation model.\n",
    "\n",
    "    Args:\n",
    "        output_tensor (torch.Tensor): Output tensor from the model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Segmented image as a NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensor to NumPy array\n",
    "    output_array = output_tensor.squeeze(0).detach().cpu().numpy()  # Shape: (1, 256, 256)\n",
    "\n",
    "    # Binarize the output (thresholding)\n",
    "    output_array = (output_array > 0.5).astype(np.uint8)  # Adjust threshold as necessary\n",
    "\n",
    "    return output_array * 255  # Scale to [0, 255] for visualization\n",
    "\n",
    "# Load and preprocess the DICOM image\n",
    "input_image_path = r'C:\\Users\\sujal\\PycharmProjects\\DSU\\dicom_communication\\0.dcm'  # Replace with your DICOM image path\n",
    "input_tensor = preprocess_dicom_image(input_image_path)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Postprocess the output\n",
    "segmented_image = postprocess_output(output_tensor)\n",
    "\n",
    "# Display or save the segmented image\n",
    "segmented_image_pil = Image.fromarray(segmented_image[0])  # Convert to PIL Image\n",
    "segmented_image_pil.show()  # Show the image"
   ],
   "id": "2063fa982acfd56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "Input shape: torch.Size([1, 3, 256, 256])\n",
      "After encoder1: torch.Size([1, 64, 256, 256])\n",
      "After encoder2: torch.Size([1, 128, 128, 128])\n",
      "After encoder3: torch.Size([1, 256, 64, 64])\n",
      "After encoder4: torch.Size([1, 512, 32, 32])\n",
      "After bottleneck: torch.Size([1, 1024, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [512, 512, 2, 2], expected input[1, 1024, 16, 16] to have 512 channels, but got 1024 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 53\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Perform inference\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():  \u001B[38;5;66;03m# Disable gradient calculation\u001B[39;00m\n\u001B[1;32m---> 53\u001B[0m     output_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# Postprocess the output\u001B[39;00m\n\u001B[0;32m     56\u001B[0m segmented_image \u001B[38;5;241m=\u001B[39m postprocess_output(output_tensor)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[47], line 54\u001B[0m, in \u001B[0;36mUNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     51\u001B[0m bottleneck \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbottleneck(F\u001B[38;5;241m.\u001B[39mmax_pool2d(enc4, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAfter bottleneck: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbottleneck\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 54\u001B[0m dec4 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbottleneck\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAfter decoder4 (before concat): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdec4\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     56\u001B[0m dec4 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((dec4, enc4), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:948\u001B[0m, in \u001B[0;36mConvTranspose2d.forward\u001B[1;34m(self, input, output_size)\u001B[0m\n\u001B[0;32m    943\u001B[0m num_spatial_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    944\u001B[0m output_padding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_padding(\n\u001B[0;32m    945\u001B[0m     \u001B[38;5;28minput\u001B[39m, output_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    946\u001B[0m     num_spatial_dims, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_transpose2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_padding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Given transposed=1, weight of size [512, 512, 2, 2], expected input[1, 1024, 16, 16] to have 512 channels, but got 1024 channels instead"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T17:24:10.463635Z",
     "start_time": "2024-09-25T17:24:10.457294Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1802efc4d9e55a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "734abb8388d1a25a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
