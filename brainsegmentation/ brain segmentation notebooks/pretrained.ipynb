{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-24T17:33:00.952585Z",
     "start_time": "2024-09-24T17:32:49.286173Z"
    }
   },
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True,trust_repo=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sujal/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:33:22.256727Z",
     "start_time": "2024-09-24T17:33:10.453503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"TCGA_CS_4944.png\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ],
   "id": "51c5a003ef872083",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:47:24.634803Z",
     "start_time": "2024-09-24T17:47:24.507923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "# Convert the image to a NumPy array for calculating mean and std\n",
    "input_image_np = np.array(input_image)\n",
    "\n",
    "# Calculate mean and std (for each channel)\n",
    "m, s = np.mean(input_image_np, axis=(0, 1)), np.std(input_image_np, axis=(0, 1))\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=m / 255.0, std=s / 255.0),  # Normalize expects values between 0 and 1\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Check if GPU is available and move data/model to GPU if so\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Print the rounded output\n",
    "print(torch.round(output[0]))\n"
   ],
   "id": "a750358ec414a311",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fn\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapper\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;129;43m@register_meta\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroi_align\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43mmeta_roi_align\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooled_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_ratio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maligned\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrois must have shape as Tensor[K, 5]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrois\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001B[0m, in \u001B[0;36mregister_meta.<locals>.wrapper\u001B[1;34m(fn)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(fn):\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension\u001B[49m\u001B[38;5;241m.\u001B[39m_has_ops():\n\u001B[0;32m     19\u001B[0m         get_meta_lib()\u001B[38;5;241m.\u001B[39mimpl(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mtorchvision, op_name), overload_name), fn)\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:02:48.691772Z",
     "start_time": "2024-09-25T08:02:48.653549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from time import time\n",
    "from nnunetv2.model_sharing.model_import import install_model_from_zip_file\n",
    "from nnunetv2.paths import nnUNet_results\n",
    "from tqdm import tqdm"
   ],
   "id": "878cf4025643ed23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:03:35.581792Z",
     "start_time": "2024-09-25T08:03:35.564693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_and_install_from_url(url):\n",
    "    assert nnUNet_results is not None, \"Cannot install model because network_training_output_dir is not \" \\\n",
    "                                                    \"set (RESULTS_FOLDER missing as environment variable, see \" \\\n",
    "                                                    \"Installation instructions)\"\n",
    "    print('Downloading pretrained model from url:', url)\n",
    "    import http.client\n",
    "    http.client.HTTPConnection._http_vsn = 10\n",
    "    http.client.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "\n",
    "    import os\n",
    "    home = os.path.expanduser('~')\n",
    "    random_number = int(time() * 1e7)\n",
    "    tempfile = join(home, f'.nnunetdownload_{str(random_number)}')\n",
    "\n",
    "    try:\n",
    "        download_file(url=url, local_filename=tempfile, chunk_size=8192 * 16)\n",
    "        print(\"Download finished. Extracting...\")\n",
    "        install_model_from_zip_file(tempfile)\n",
    "        print(\"Done\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        if isfile(tempfile):\n",
    "            os.remove(tempfile)"
   ],
   "id": "907ada2ce5350fbb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:03:54.422061Z",
     "start_time": "2024-09-25T08:03:54.409565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_file(url: str, local_filename: str, chunk_size: Optional[int] = 8192 * 16) -> str:\n",
    "    # borrowed from https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True, timeout=100) as r:\n",
    "        r.raise_for_status()\n",
    "        with tqdm.wrapattr(open(local_filename, 'wb'), \"write\", total=int(r.headers.get(\"Content-Length\"))) as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ],
   "id": "8492d3720fac9a5b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T11:35:14.248615Z",
     "start_time": "2024-09-25T11:35:13.751559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        self.decoder4 = self.upconv_block(1024, 512)\n",
    "        self.decoder3 = self.upconv_block(512, 256)\n",
    "        self.decoder2 = self.upconv_block(256, 128)\n",
    "        self.decoder1 = self.upconv_block(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2))\n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc4, kernel_size=2))\n",
    "        dec4 = self.decoder4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder3(dec4)\n",
    "        dec4 = torch.cat((dec4, enc3), dim=1)\n",
    "        dec3 = self.decoder2(dec4)\n",
    "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
    "        dec2 = self.decoder1(dec3)\n",
    "        dec2 = torch.cat((dec2, enc1), dim=1)\n",
    "        return self.final_conv(dec2)\n",
    "\n",
    "# Initialize the model and load the weights\n",
    "model = UNet(in_channels=3, out_channels=1)  # Adjust channels as needed\n",
    "model.load_state_dict(torch.load('3channel50.pt', map_location=torch.device('cpu')),strict=False)\n",
    "model.eval()\n"
   ],
   "id": "a3f8f511e0ba3602",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujal\\AppData\\Local\\Temp\\ipykernel_2332\\2642201277.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('3channel50.pt', map_location=torch.device('cpu')),strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder4): Sequential(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder1): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T11:40:49.206554Z",
     "start_time": "2024-09-25T11:40:49.182294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "def preprocess_image(image_path):\n",
    "    # Load image using OpenCV (change this if you use PIL)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image if necessary\n",
    "    image = cv2.resize(image, (256, 256))  # Change to your required input size\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Shape: (1, 3, 256, 256)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "# Function to postprocess the output\n",
    "def postprocess_output(output_tensor):\n",
    "    # Convert tensor to numpy array\n",
    "    output_array = output_tensor.squeeze(0).detach().numpy()  # Shape: (1, 256, 256)\n",
    "    \n",
    "    # Binarize the output (thresholding)\n",
    "    output_array = (output_array > 0.5).astype(np.uint8)  # Adjust threshold as necessary\n",
    "    \n",
    "    return output_array * 255  # Scale to [0, 255] for visualization\n",
    "\n",
    "# Load and preprocess the image\n",
    "input_image_path = r'C:\\Users\\sujal\\PycharmProjects\\DSU\\dicom_communication\\0.dcm'  # Replace with your image path\n",
    "input_tensor = preprocess_image(input_image_path)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Postprocess the output\n",
    "segmented_image = postprocess_output(output_tensor)\n",
    "\n",
    "# Display or save the segmented image\n",
    "segmented_image_pil = Image.fromarray(segmented_image[0])  # Convert to PIL Image\n",
    "segmented_image_pil.show()  # Show the image"
   ],
   "id": "2063fa982acfd56f",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess_image\u001B[39m(image_path):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# Load image using OpenCV (change this if you use PIL)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(image_path)\n",
      "\u001B[1;31mImportError\u001B[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1802efc4d9e55a5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
